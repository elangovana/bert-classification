{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert text classification on SST2 using PyTorch\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sagemaker==2.9.1 in /Users/aeg/envs/bert-sst2-bc3/lib/python3.7/site-packages (from -r requirements_notebook.txt (line 1)) (2.9.1)\n",
      "Requirement already satisfied: boto3==1.16.1 in /Users/aeg/envs/bert-sst2-bc3/lib/python3.7/site-packages (from -r requirements_notebook.txt (line 2)) (1.16.1)\n",
      "Requirement already satisfied: scikit-learn==0.23.1 in /Users/aeg/envs/bert-sst2-bc3/lib/python3.7/site-packages (from -r requirements_notebook.txt (line 3)) (0.23.1)\n",
      "Requirement already satisfied: botocore<1.20.0,>=1.19.1 in /Users/aeg/envs/bert-sst2-bc3/lib/python3.7/site-packages (from boto3==1.16.1->-r requirements_notebook.txt (line 2)) (1.19.56)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /Users/aeg/envs/bert-sst2-bc3/lib/python3.7/site-packages (from boto3==1.16.1->-r requirements_notebook.txt (line 2)) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /Users/aeg/envs/bert-sst2-bc3/lib/python3.7/site-packages (from boto3==1.16.1->-r requirements_notebook.txt (line 2)) (0.3.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/aeg/envs/bert-sst2-bc3/lib/python3.7/site-packages (from sagemaker==2.9.1->-r requirements_notebook.txt (line 1)) (20.8)\n",
      "Requirement already satisfied: smdebug-rulesconfig==0.1.5 in /Users/aeg/envs/bert-sst2-bc3/lib/python3.7/site-packages (from sagemaker==2.9.1->-r requirements_notebook.txt (line 1)) (0.1.5)\n",
      "Requirement already satisfied: importlib-metadata>=1.4.0 in /Users/aeg/envs/bert-sst2-bc3/lib/python3.7/site-packages (from sagemaker==2.9.1->-r requirements_notebook.txt (line 1)) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.9.0 in /Users/aeg/envs/bert-sst2-bc3/lib/python3.7/site-packages (from sagemaker==2.9.1->-r requirements_notebook.txt (line 1)) (1.19.5)\n",
      "Requirement already satisfied: protobuf>=3.1 in /Users/aeg/envs/bert-sst2-bc3/lib/python3.7/site-packages (from sagemaker==2.9.1->-r requirements_notebook.txt (line 1)) (3.14.0)\n",
      "Requirement already satisfied: google-pasta in /Users/aeg/envs/bert-sst2-bc3/lib/python3.7/site-packages (from sagemaker==2.9.1->-r requirements_notebook.txt (line 1)) (0.2.0)\n",
      "Requirement already satisfied: protobuf3-to-dict>=0.1.5 in /Users/aeg/envs/bert-sst2-bc3/lib/python3.7/site-packages (from sagemaker==2.9.1->-r requirements_notebook.txt (line 1)) (0.1.5)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/aeg/envs/bert-sst2-bc3/lib/python3.7/site-packages (from scikit-learn==0.23.1->-r requirements_notebook.txt (line 3)) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/aeg/envs/bert-sst2-bc3/lib/python3.7/site-packages (from scikit-learn==0.23.1->-r requirements_notebook.txt (line 3)) (1.0.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /Users/aeg/envs/bert-sst2-bc3/lib/python3.7/site-packages (from scikit-learn==0.23.1->-r requirements_notebook.txt (line 3)) (1.6.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /Users/aeg/envs/bert-sst2-bc3/lib/python3.7/site-packages (from botocore<1.20.0,>=1.19.1->boto3==1.16.1->-r requirements_notebook.txt (line 2)) (1.26.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /Users/aeg/envs/bert-sst2-bc3/lib/python3.7/site-packages (from botocore<1.20.0,>=1.19.1->boto3==1.16.1->-r requirements_notebook.txt (line 2)) (2.8.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/aeg/envs/bert-sst2-bc3/lib/python3.7/site-packages (from importlib-metadata>=1.4.0->sagemaker==2.9.1->-r requirements_notebook.txt (line 1)) (3.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /Users/aeg/envs/bert-sst2-bc3/lib/python3.7/site-packages (from importlib-metadata>=1.4.0->sagemaker==2.9.1->-r requirements_notebook.txt (line 1)) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/aeg/envs/bert-sst2-bc3/lib/python3.7/site-packages (from packaging>=20.0->sagemaker==2.9.1->-r requirements_notebook.txt (line 1)) (2.4.7)\n",
      "Requirement already satisfied: six>=1.9 in /Users/aeg/envs/bert-sst2-bc3/lib/python3.7/site-packages (from protobuf>=3.1->sagemaker==2.9.1->-r requirements_notebook.txt (line 1)) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements_notebook.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, shutil\n",
    "import logging\n",
    "\n",
    "sys.path.append(\"src\")\n",
    "\n",
    "logging.basicConfig(level=\"INFO\", handlers=[logging.StreamHandler(sys.stdout)],\n",
    "                        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bucket and role set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-21 19:06:52,364 - sagemaker.analytics - WARNING - pandas failed to import. Analytics features will be impaired or broken.\n",
      "2021-01-21 19:06:52,467 - botocore.credentials - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2021-01-21 19:06:52,603 - botocore.credentials - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2021-01-21 19:06:52,666 - botocore.credentials - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n"
     ]
    }
   ],
   "source": [
    "import sagemaker, boto3\n",
    "from sagemaker import get_execution_role\n",
    "sm_session = sagemaker.session.Session()\n",
    "\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "account_id =  boto3.client('sts').get_caller_identity().get('Account')\n",
    "region = boto3.session.Session().region_name\n",
    "\n",
    "\n",
    "#role = sagemaker.get_execution_role()\n",
    "role=\"arn:aws:iam::{}:role/service-role/AmazonSageMaker-ExecutionRole-20190118T115449\".format(account_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_from_checkpoint=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bucket = sm_session.default_bucket()\n",
    "\n",
    "data_bucket_prefix = \"bert-sst2-classify\"\n",
    "\n",
    "s3_uri_data = \"s3://{}/{}/data\".format(data_bucket, data_bucket_prefix)\n",
    "s3_uri_train = \"{}/{}\".format(s3_uri_data, \"train.csv\")\n",
    "s3_uri_val = \"{}/{}\".format(s3_uri_data, \"dev.csv\")\n",
    "\n",
    "s3_uri_test = \"{}/{}\".format(s3_uri_data, \"test.csv\")\n",
    "\n",
    "s3_output_path = \"s3://{}/{}/output\".format(data_bucket, data_bucket_prefix)\n",
    "s3_code_path = \"s3://{}/{}/code\".format(data_bucket, data_bucket_prefix)\n",
    "s3_checkpoint = \"s3://{}/{}/checkpoint\".format(data_bucket, data_bucket_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "def delete_s3_objects(s3_uri):\n",
    "    client = boto3.client('s3')\n",
    "    s3_uri = s3_uri.replace(\"s3://\",\"\")\n",
    "    bucket, prefix = s3_uri.split(\"/\")[0], \"/\".join( s3_uri.split(\"/\")[1:])\n",
    "    \n",
    "    response = client.list_objects(\n",
    "    Bucket=bucket,\n",
    "    Delimiter='|',\n",
    "    Prefix=prefix,\n",
    "    MaxKeys = 20\n",
    ")\n",
    "    s3 = boto3.resource('s3')\n",
    "    for item in response.get(\"Contents\", []):\n",
    "        print(\"Deleting {}\".format(item[\"Key\"]))\n",
    "        obj = s3.Object(bucket, item[\"Key\"] )\n",
    "        obj.delete()\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not resume_from_checkpoint:\n",
    "    delete_s3_objects(s3_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_dataset = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_dir = \"tmp\"\n",
    "processed_out_dir = os.path.join(raw_data_dir, \"processd\")\n",
    "\n",
    "if os.path.exists(processed_out_dir):\n",
    "    shutil.rmtree(processed_out_dir)\n",
    "\n",
    "os.makedirs(processed_out_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-2-324346001917/bert-sst2-classify/data/train.csv'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3_uri_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen \n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "import shutil, glob\n",
    "\n",
    "def download_glue_sst2(output_dir, glue_sst2_uri=\"https://dl.fbaipublicfiles.com/glue/data/SST-2.zip\"):\n",
    "    resp = urlopen(glue_sst2_uri)\n",
    "    zip_file = ZipFile(BytesIO(resp.read()))\n",
    "    zip_file.extractall(output_dir)\n",
    "    for f in glob.glob(\"{}/SST-2/*.tsv\".format(output_dir)):\n",
    "        shutil.move(f, output_dir)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading file tmp/processd/train.tsv to s3://sagemaker-us-east-2-324346001917/bert-sst2-classify/data/train.csv in 11.44324 seconds\n",
      "Uploading file tmp/processd/dev.tsv to s3://sagemaker-us-east-2-324346001917/bert-sst2-classify/data/dev.csv in 4.87655 seconds\n",
      "Uploading file tmp/processd/test.tsv to s3://sagemaker-us-east-2-324346001917/bert-sst2-classify/data/test.csv in 5.758917 seconds\n"
     ]
    }
   ],
   "source": [
    "#from utils.sst2_split_utils import SST2SplitUtils\n",
    "from s3_util import S3Util\n",
    "\n",
    "if prepare_dataset:\n",
    "#     input_file = os.path.join(raw_data_dir , \"datasetSentences.txt\")\n",
    "#     split_file = os.path.join(raw_data_dir , \"datasetSplit.txt\")\n",
    "#     dictionary = os.path.join(raw_data_dir , \"dictionary.txt\")\n",
    "#     label_file = os.path.join(raw_data_dir , \"sentiment_labels.txt\")\n",
    "#     SST2SplitUtils().split( input_file, split_file, dictionary, label_file, processed_out_dir)\n",
    "\n",
    "    \n",
    "    download_glue_sst2(processed_out_dir)\n",
    "    S3Util().upload_file(os.path.join(processed_out_dir, \"train.tsv\"), s3_uri_train )\n",
    "    S3Util().upload_file(os.path.join(processed_out_dir, \"dev.tsv\"), s3_uri_val )\n",
    "    S3Util().upload_file(os.path.join(processed_out_dir, \"test.tsv\"), s3_uri_test )\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "This shows you how to train BERT on SageMaker using SPOT instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_full =  {\n",
    "    \"train\" : s3_uri_train,\n",
    "    \"val\" : s3_uri_val,\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "inputs = inputs_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_localcheckpoint_dir=\"/opt/ml/checkpoints/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_type = \"ml.p3.2xlarge\"\n",
    "instance_type_gpu_map = {\"ml.p3.8xlarge\":4, \"ml.p3.2xlarge\": 1, \"ml.p3.16xlarge\":8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp = {\n",
    "\"epochs\" : 30,\n",
    "\"earlystoppingpatience\" : 3,\n",
    "# Increasing batch size might end up with CUDA OOM error, increase grad accumulation instead\n",
    "\"batch\" : 8 * instance_type_gpu_map[instance_type],\n",
    "\"trainfile\" :s3_uri_train.split(\"/\")[-1],\n",
    "\"valfile\" : s3_uri_val.split(\"/\")[-1],\n",
    "\"datasetfactory\":\"datasets.sst2_dataset_factory.SST2DatasetFactory\",\n",
    "# The number of steps to accumulate gradients for\n",
    "\"gradaccumulation\" : 4,\n",
    "\"log-level\":\"INFO\",\n",
    "# This param depends on your model max pos embedding size or when large you might end up with CUDA OOM error    \n",
    "\"maxseqlen\" : 512,\n",
    "# Make sure the lr is quite small, as this is a pretrained model..\n",
    "\"lr\":0.00001,\n",
    "# Use finetuning (set to 1), if you only want to change the weights in the final classification layer.. \n",
    "\"finetune\": 0,\n",
    "\"checkpointdir\" : sm_localcheckpoint_dir,\n",
    "# Checkpoints once every n epochs\n",
    "\"checkpointfreq\": 2\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'epochs': 30,\n",
       " 'earlystoppingpatience': 3,\n",
       " 'batch': 8,\n",
       " 'trainfile': 'train.csv',\n",
       " 'valfile': 'dev.csv',\n",
       " 'datasetfactory': 'datasets.sst2_dataset_factory.SST2DatasetFactory',\n",
       " 'gradaccumulation': 4,\n",
       " 'log-level': 'INFO',\n",
       " 'maxseqlen': 512,\n",
       " 'lr': 1e-05,\n",
       " 'finetune': 0,\n",
       " 'checkpointdir': '/opt/ml/checkpoints/',\n",
       " 'checkpointfreq': 2}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 's3://sagemaker-us-east-2-324346001917/bert-sst2-classify/data/train.csv',\n",
       " 'val': 's3://sagemaker-us-east-2-324346001917/bert-sst2-classify/data/dev.csv'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_definitions = [{\"Name\": \"TrainLoss\",\n",
    "                     \"Regex\": \"###score: train_loss### (\\d*[.]?\\d*)\"}\n",
    "                    ,{\"Name\": \"ValidationLoss\",\n",
    "                     \"Regex\": \"###score: val_loss### (\\d*[.]?\\d*)\"}\n",
    "                    ,{\"Name\": \"TrainScore\",\n",
    "                     \"Regex\": \"###score: train_score### (\\d*[.]?\\d*)\"}\n",
    "                   ,{\"Name\": \"ValidationScore\",\n",
    "                     \"Regex\": \"###score: val_score### (\\d*[.]?\\d*)\"}\n",
    "                    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set True if you need spot instance\n",
    "use_spot = True\n",
    "train_max_run_secs =   2*24 * 60 * 60\n",
    "spot_wait_sec =  5 * 60\n",
    "max_wait_time_secs = train_max_run_secs +  spot_wait_sec\n",
    "\n",
    "if not use_spot:\n",
    "    max_wait_time_secs = None\n",
    "    s3_checkpoint=None\n",
    "    sm_localcheckpoint_dir=None\n",
    "    hp.pop(\"checkpointdir\")\n",
    "    \n",
    "# During local mode, no spot.., use smaller dataset\n",
    "if instance_type == 'local':\n",
    "    use_spot = False\n",
    "    max_wait_time_secs = 0\n",
    "    wait = True\n",
    "    # Use smaller dataset to run locally\n",
    "    inputs = inputs_sample\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_type = \"bert-sst2-classification\"\n",
    "base_name = \"{}\".format(job_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-21 19:07:26,486 - sagemaker - INFO - Creating training-job with name: bert-sst2-classification-2021-01-21-08-07-24-370\n",
      "2021-01-21 08:07:27 Starting - Starting the training job...\n",
      "2021-01-21 08:07:29 Starting - Launching requested ML instances...\n",
      "2021-01-21 08:08:32 Starting - Preparing the instances for training......\n",
      "2021-01-21 08:09:33 Downloading - Downloading input data...\n",
      "2021-01-21 08:09:54 Training - Downloading the training image......\n",
      "2021-01-21 08:11:23 Training - Training image download completed. Training in progress.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2021-01-21 08:11:22,938 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2021-01-21 08:11:22,964 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2021-01-21 08:11:22,969 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2021-01-21 08:11:23,279 sagemaker-containers INFO     Module default_user_module_name does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2021-01-21 08:11:23,280 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2021-01-21 08:11:23,280 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2021-01-21 08:11:23,280 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python -m pip install . -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing /tmp/tmp15eulo55/module_dir\u001b[0m\n",
      "\u001b[34mCollecting transformers==2.3.0\n",
      "  Downloading transformers-2.3.0-py3-none-any.whl (447 kB)\u001b[0m\n",
      "\u001b[34mCollecting sentencepiece==0.1.91\n",
      "  Downloading sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1 MB)\u001b[0m\n",
      "\u001b[34mCollecting scikit-learn==0.23.1\n",
      "  Downloading scikit_learn-0.23.1-cp36-cp36m-manylinux1_x86_64.whl (6.8 MB)\u001b[0m\n",
      "\u001b[34mCollecting sacremoses\n",
      "  Downloading sacremoses-0.0.43.tar.gz (883 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm in /opt/conda/lib/python3.6/site-packages (from transformers==2.3.0->-r requirements.txt (line 1)) (4.42.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from transformers==2.3.0->-r requirements.txt (line 1)) (1.16.4)\u001b[0m\n",
      "\u001b[34mCollecting regex!=2019.12.17\n",
      "  Downloading regex-2020.11.13-cp36-cp36m-manylinux2014_x86_64.whl (723 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from transformers==2.3.0->-r requirements.txt (line 1)) (2.22.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: boto3 in /opt/conda/lib/python3.6/site-packages (from transformers==2.3.0->-r requirements.txt (line 1)) (1.13.23)\u001b[0m\n",
      "\u001b[34mCollecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-2.1.0-py3-none-any.whl (12 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.6/site-packages (from scikit-learn==0.23.1->-r requirements.txt (line 3)) (0.15.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.6/site-packages (from scikit-learn==0.23.1->-r requirements.txt (line 3)) (1.2.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==2.3.0->-r requirements.txt (line 1)) (1.14.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==2.3.0->-r requirements.txt (line 1)) (7.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==2.3.0->-r requirements.txt (line 1)) (1.25.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==2.3.0->-r requirements.txt (line 1)) (2.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==2.3.0->-r requirements.txt (line 1)) (2020.4.5.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==2.3.0->-r requirements.txt (line 1)) (3.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /opt/conda/lib/python3.6/site-packages (from boto3->transformers==2.3.0->-r requirements.txt (line 1)) (0.3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.6/site-packages (from boto3->transformers==2.3.0->-r requirements.txt (line 1)) (0.10.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: botocore<1.17.0,>=1.16.23 in /opt/conda/lib/python3.6/site-packages (from boto3->transformers==2.3.0->-r requirements.txt (line 1)) (1.16.23)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.6/site-packages (from botocore<1.17.0,>=1.16.23->boto3->transformers==2.3.0->-r requirements.txt (line 1)) (2.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: docutils<0.16,>=0.10 in /opt/conda/lib/python3.6/site-packages (from botocore<1.17.0,>=1.16.23->boto3->transformers==2.3.0->-r requirements.txt (line 1)) (0.15.2)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: default-user-module-name, sacremoses\n",
      "  Building wheel for default-user-module-name (setup.py): started\n",
      "  Building wheel for default-user-module-name (setup.py): finished with status 'done'\n",
      "  Created wheel for default-user-module-name: filename=default_user_module_name-1.0.0-py2.py3-none-any.whl size=32265 sha256=8cfd036da4a9f475d1408e5a22c3c184f4e352d94109a1e09ff8724468ddd142\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-hhp_qziq/wheels/be/c9/00/846460b6c9db6166ffb3fe15eecf066594045ca934f8969e07\n",
      "  Building wheel for sacremoses (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for sacremoses (setup.py): finished with status 'done'\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.43-py3-none-any.whl size=893259 sha256=f31a1f21f04f22f32ee84d40a04c8d09d0c045be5f2db66276919fa179c78345\n",
      "  Stored in directory: /root/.cache/pip/wheels/49/25/98/cdea9c79b2d9a22ccc59540b1784b67f06b633378e97f58da2\u001b[0m\n",
      "\u001b[34mSuccessfully built default-user-module-name sacremoses\u001b[0m\n",
      "\u001b[34mInstalling collected packages: regex, sacremoses, sentencepiece, transformers, threadpoolctl, scikit-learn, default-user-module-name\u001b[0m\n",
      "\u001b[34m  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 0.21.2\n",
      "    Uninstalling scikit-learn-0.21.2:\n",
      "      Successfully uninstalled scikit-learn-0.21.2\u001b[0m\n",
      "\u001b[34mSuccessfully installed default-user-module-name-1.0.0 regex-2020.11.13 sacremoses-0.0.43 scikit-learn-0.23.1 sentencepiece-0.1.91 threadpoolctl-2.1.0 transformers-2.3.0\u001b[0m\n",
      "\u001b[34mWARNING: You are using pip version 20.1.1; however, version 20.3.3 is available.\u001b[0m\n",
      "\u001b[34mYou should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[34m2021-01-21 08:11:32,353 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"val\": \"/opt/ml/input/data/val\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"checkpointfreq\": 2,\n",
      "        \"lr\": 1e-05,\n",
      "        \"batch\": 8,\n",
      "        \"trainfile\": \"train.csv\",\n",
      "        \"gradaccumulation\": 4,\n",
      "        \"datasetfactory\": \"datasets.sst2_dataset_factory.SST2DatasetFactory\",\n",
      "        \"finetune\": 0,\n",
      "        \"log-level\": \"INFO\",\n",
      "        \"maxseqlen\": 512,\n",
      "        \"valfile\": \"dev.csv\",\n",
      "        \"epochs\": 30,\n",
      "        \"earlystoppingpatience\": 3,\n",
      "        \"checkpointdir\": \"/opt/ml/checkpoints/\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"val\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"bert-sst2-classification-2021-01-21-08-07-24-370\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-2-324346001917/bert-sst2-classify/code/bert-sst2-classification-2021-01-21-08-07-24-370/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"main\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"main.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch\":8,\"checkpointdir\":\"/opt/ml/checkpoints/\",\"checkpointfreq\":2,\"datasetfactory\":\"datasets.sst2_dataset_factory.SST2DatasetFactory\",\"earlystoppingpatience\":3,\"epochs\":30,\"finetune\":0,\"gradaccumulation\":4,\"log-level\":\"INFO\",\"lr\":1e-05,\"maxseqlen\":512,\"trainfile\":\"train.csv\",\"valfile\":\"dev.csv\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=main.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"val\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\",\"val\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=main\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-2-324346001917/bert-sst2-classify/code/bert-sst2-classification-2021-01-21-08-07-24-370/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\",\"val\":\"/opt/ml/input/data/val\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch\":8,\"checkpointdir\":\"/opt/ml/checkpoints/\",\"checkpointfreq\":2,\"datasetfactory\":\"datasets.sst2_dataset_factory.SST2DatasetFactory\",\"earlystoppingpatience\":3,\"epochs\":30,\"finetune\":0,\"gradaccumulation\":4,\"log-level\":\"INFO\",\"lr\":1e-05,\"maxseqlen\":512,\"trainfile\":\"train.csv\",\"valfile\":\"dev.csv\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"val\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"bert-sst2-classification-2021-01-21-08-07-24-370\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-2-324346001917/bert-sst2-classify/code/bert-sst2-classification-2021-01-21-08-07-24-370/source/sourcedir.tar.gz\",\"module_name\":\"main\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"main.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch\",\"8\",\"--checkpointdir\",\"/opt/ml/checkpoints/\",\"--checkpointfreq\",\"2\",\"--datasetfactory\",\"datasets.sst2_dataset_factory.SST2DatasetFactory\",\"--earlystoppingpatience\",\"3\",\"--epochs\",\"30\",\"--finetune\",\"0\",\"--gradaccumulation\",\"4\",\"--log-level\",\"INFO\",\"--lr\",\"1e-05\",\"--maxseqlen\",\"512\",\"--trainfile\",\"train.csv\",\"--valfile\",\"dev.csv\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_VAL=/opt/ml/input/data/val\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_CHECKPOINTFREQ=2\u001b[0m\n",
      "\u001b[34mSM_HP_LR=1e-05\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH=8\u001b[0m\n",
      "\u001b[34mSM_HP_TRAINFILE=train.csv\u001b[0m\n",
      "\u001b[34mSM_HP_GRADACCUMULATION=4\u001b[0m\n",
      "\u001b[34mSM_HP_DATASETFACTORY=datasets.sst2_dataset_factory.SST2DatasetFactory\u001b[0m\n",
      "\u001b[34mSM_HP_FINETUNE=0\u001b[0m\n",
      "\u001b[34mSM_HP_LOG-LEVEL=INFO\u001b[0m\n",
      "\u001b[34mSM_HP_MAXSEQLEN=512\u001b[0m\n",
      "\u001b[34mSM_HP_VALFILE=dev.csv\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=30\u001b[0m\n",
      "\u001b[34mSM_HP_EARLYSTOPPINGPATIENCE=3\u001b[0m\n",
      "\u001b[34mSM_HP_CHECKPOINTDIR=/opt/ml/checkpoints/\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python main.py --batch 8 --checkpointdir /opt/ml/checkpoints/ --checkpointfreq 2 --datasetfactory datasets.sst2_dataset_factory.SST2DatasetFactory --earlystoppingpatience 3 --epochs 30 --finetune 0 --gradaccumulation 4 --log-level INFO --lr 1e-05 --maxseqlen 512 --trainfile train.csv --valfile dev.csv\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34m{'trainfile': 'train.csv', 'traindir': '/opt/ml/input/data/train', 'valfile': 'dev.csv', 'valdir': '/opt/ml/input/data/val', 'datasetfactory': 'datasets.sst2_dataset_factory.SST2DatasetFactory', 'outdir': '/opt/ml/output/data', 'modeldir': '/opt/ml/model', 'checkpointdir': '/opt/ml/checkpoints/', 'checkpointfreq': '2', 'earlystoppingpatience': 3, 'epochs': 30, 'gradaccumulation': 4, 'batch': 8, 'lr': 1e-05, 'finetune': 0, 'maxseqlen': 512, 'log_level': 'INFO'}\u001b[0m\n",
      "\u001b[34m2021-01-21 08:11:34,977 - builder - INFO - Retrieving Tokeniser\u001b[0m\n",
      "\u001b[34m2021-01-21 08:11:35,070 - transformers.file_utils - INFO - https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt not found in cache or force_download set to True, downloading to /tmp/tmpifw_t3nb\u001b[0m\n",
      "\u001b[34m2021-01-21 08:11:35,192 - transformers.file_utils - INFO - copying /tmp/tmpifw_t3nb to cache at /root/.cache/torch/transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\u001b[0m\n",
      "\u001b[34m2021-01-21 08:11:35,192 - transformers.file_utils - INFO - creating metadata file for /root/.cache/torch/transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\u001b[0m\n",
      "\u001b[34m2021-01-21 08:11:35,193 - transformers.file_utils - INFO - removing temp file /tmp/tmpifw_t3nb\u001b[0m\n",
      "\u001b[34m2021-01-21 08:11:35,193 - transformers.tokenization_utils - INFO - loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /root/.cache/torch/transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\u001b[0m\n",
      "\u001b[34m2021-01-21 08:11:35,290 - builder - INFO - Completed retrieving Tokeniser\u001b[0m\n",
      "\u001b[34m2021-01-21 08:11:35,303 - builder - INFO - Retrieving Tokeniser\u001b[0m\n",
      "\u001b[34m2021-01-21 08:11:35,303 - builder - INFO - Completed retrieving Tokeniser\u001b[0m\n",
      "\u001b[34m2021-01-21 08:11:35,303 - datasets.sst2_dataset - INFO - loading /opt/ml/input/data/train/train.csv\u001b[0m\n",
      "\u001b[34m2021-01-21 08:11:35,381 - datasets.sst2_dataset - INFO - Loaded file /opt/ml/input/data/train/train.csv with 67349 records Counter({'1': 37569, '0': 29780})\u001b[0m\n",
      "\u001b[34m2021-01-21 08:11:35,381 - builder - INFO - Retrieving Tokeniser\u001b[0m\n",
      "\u001b[34m2021-01-21 08:11:35,381 - builder - INFO - Completed retrieving Tokeniser\u001b[0m\n",
      "\u001b[34m2021-01-21 08:11:35,381 - datasets.sst2_dataset - INFO - loading /opt/ml/input/data/val/dev.csv\u001b[0m\n",
      "\u001b[34m2021-01-21 08:11:35,383 - datasets.sst2_dataset - INFO - Loaded file /opt/ml/input/data/val/dev.csv with 872 records Counter({'1': 444, '0': 428})\u001b[0m\n",
      "\u001b[34m2021-01-21 08:11:35,383 - builder - INFO - Retrieving model\u001b[0m\n",
      "\u001b[34m2021-01-21 08:11:35,458 - transformers.file_utils - INFO - https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json not found in cache or force_download set to True, downloading to /tmp/tmpdu0wuzh1\u001b[0m\n",
      "\u001b[34m2021-01-21 08:11:35,545 - transformers.file_utils - INFO - copying /tmp/tmpdu0wuzh1 to cache at /root/.cache/torch/transformers/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391\u001b[0m\n",
      "\u001b[34m2021-01-21 08:11:35,545 - transformers.file_utils - INFO - creating metadata file for /root/.cache/torch/transformers/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391\u001b[0m\n",
      "\u001b[34m2021-01-21 08:11:35,546 - transformers.file_utils - INFO - removing temp file /tmp/tmpdu0wuzh1\u001b[0m\n",
      "\u001b[34m2021-01-21 08:11:35,546 - transformers.configuration_utils - INFO - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json from cache at /root/.cache/torch/transformers/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391\u001b[0m\n",
      "\u001b[34m2021-01-21 08:11:35,546 - transformers.configuration_utils - INFO - Model config {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 28996\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34m2021-01-21 08:11:35,631 - transformers.file_utils - INFO - https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin not found in cache or force_download set to True, downloading to /tmp/tmpzqrck28z\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2021-01-21 08:11:43,849 - transformers.file_utils - INFO - copying /tmp/tmpzqrck28z to cache at /root/.cache/torch/transformers/35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2\u001b[0m\n",
      "\u001b[34m2021-01-21 08:11:44,401 - transformers.file_utils - INFO - creating metadata file for /root/.cache/torch/transformers/35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2\u001b[0m\n",
      "\u001b[34m2021-01-21 08:11:44,402 - transformers.file_utils - INFO - removing temp file /tmp/tmpzqrck28z\u001b[0m\n",
      "\u001b[34m2021-01-21 08:11:44,475 - transformers.modeling_utils - INFO - loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin from cache at /root/.cache/torch/transformers/35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2\u001b[0m\n",
      "\u001b[34m2021-01-21 08:11:47,881 - transformers.modeling_utils - INFO - Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\u001b[0m\n",
      "\u001b[34m2021-01-21 08:11:47,881 - transformers.modeling_utils - INFO - Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\u001b[0m\n",
      "\u001b[34m2021-01-21 08:11:47,882 - builder - INFO - Retrieving model complete\u001b[0m\n",
      "\u001b[34m2021-01-21 08:50:13,378 - bert_train - INFO - Train set result details:\u001b[0m\n",
      "\u001b[34m2021-01-21 08:50:13,437 - bert_train - INFO - Train set result details: 0.8135384341267131\u001b[0m\n",
      "\u001b[34m2021-01-21 08:50:13,438 - bert_train - INFO - Validation set result details:\u001b[0m\n",
      "\u001b[34m2021-01-21 08:50:23,420 - bert_train - INFO - Validation set result details: 0.9071100917431193 \u001b[0m\n",
      "\u001b[34m2021-01-21 08:50:23,421 - bert_train - INFO - Snapshotting because the current score 0.9071100917431193 is greater than None \u001b[0m\n",
      "\u001b[34m2021-01-21 08:50:23,421 - bert_train - INFO - Snapshot model to /opt/ml/model/best_snaphsotmodel.pt\u001b[0m\n",
      "\u001b[34m2021-01-21 08:50:23,970 - bert_train - INFO - Checkpoint model to /opt/ml/checkpoints/checkpoint.pt\u001b[0m\n",
      "\u001b[34m2021-01-21 08:50:24,449 - bert_train - INFO - Run   2316     0      8419     6/8419        0% 0.043854 0.030009       0.8135       0.9071\u001b[0m\n",
      "\u001b[34m###score: train_loss### 0.04385380411744277\u001b[0m\n",
      "\u001b[34m###score: val_loss### 0.030009197395875913\u001b[0m\n",
      "\u001b[34m###score: train_score### 0.8135384341267131\u001b[0m\n",
      "\u001b[34m###score: val_score### 0.9071100917431193\u001b[0m\n",
      "\u001b[34m2021-01-21 09:28:48,220 - bert_train - INFO - Train set result details:\u001b[0m\n",
      "\u001b[34m2021-01-21 09:28:48,261 - bert_train - INFO - Train set result details: 0.9483140061470846\u001b[0m\n",
      "\u001b[34m2021-01-21 09:28:48,262 - bert_train - INFO - Validation set result details:\u001b[0m\n",
      "\u001b[34m2021-01-21 09:28:58,204 - bert_train - INFO - Validation set result details: 0.9197247706422018 \u001b[0m\n",
      "\u001b[34m2021-01-21 09:28:58,204 - bert_train - INFO - Snapshotting because the current score 0.9197247706422018 is greater than 0.9071100917431193 \u001b[0m\n",
      "\u001b[34m2021-01-21 09:28:58,204 - bert_train - INFO - Snapshot model to /opt/ml/model/best_snaphsotmodel.pt\u001b[0m\n",
      "\u001b[34m2021-01-21 09:28:59,137 - bert_train - INFO - Checkpoint model to /opt/ml/checkpoints/checkpoint.pt\u001b[0m\n",
      "\u001b[34m2021-01-21 09:29:00,926 - bert_train - INFO - Run   4633     1     16838     6/8419        0% 0.017783 0.029534       0.9483       0.9197\u001b[0m\n",
      "\u001b[34m###score: train_loss### 0.017782839655956693\u001b[0m\n",
      "\u001b[34m###score: val_loss### 0.029533852716275583\u001b[0m\n",
      "\u001b[34m###score: train_score### 0.9483140061470846\u001b[0m\n",
      "\u001b[34m###score: val_score### 0.9197247706422018\u001b[0m\n",
      "\u001b[34m2021-01-21 10:07:22,963 - bert_train - INFO - Train set result details:\u001b[0m\n",
      "\u001b[34m2021-01-21 10:07:23,005 - bert_train - INFO - Train set result details: 0.964424119140596\u001b[0m\n",
      "\u001b[34m2021-01-21 10:07:23,005 - bert_train - INFO - Validation set result details:\u001b[0m\n",
      "\u001b[34m2021-01-21 10:07:32,896 - bert_train - INFO - Validation set result details: 0.9208715596330275 \u001b[0m\n",
      "\u001b[34m2021-01-21 10:07:32,896 - bert_train - INFO - Snapshotting because the current score 0.9208715596330275 is greater than 0.9197247706422018 \u001b[0m\n",
      "\u001b[34m2021-01-21 10:07:32,896 - bert_train - INFO - Snapshot model to /opt/ml/model/best_snaphsotmodel.pt\u001b[0m\n",
      "\u001b[34m2021-01-21 10:07:33,589 - bert_train - INFO - Checkpoint model to /opt/ml/checkpoints/checkpoint.pt\u001b[0m\n",
      "\u001b[34m2021-01-21 10:07:34,189 - bert_train - INFO - Run   6946     2     25257     6/8419        0% 0.012702 0.031841       0.9644       0.9209\u001b[0m\n",
      "\u001b[34m###score: train_loss### 0.012701942957926655\u001b[0m\n",
      "\u001b[34m###score: val_loss### 0.03184102476497582\u001b[0m\n",
      "\u001b[34m###score: train_score### 0.964424119140596\u001b[0m\n",
      "\u001b[34m###score: val_score### 0.9208715596330275\u001b[0m\n",
      "\u001b[34m2021-01-21 10:45:54,433 - bert_train - INFO - Train set result details:\u001b[0m\n",
      "\u001b[34m2021-01-21 10:45:54,480 - bert_train - INFO - Train set result details: 0.9734814176899435\u001b[0m\n",
      "\u001b[34m2021-01-21 10:45:54,480 - bert_train - INFO - Validation set result details:\u001b[0m\n",
      "\u001b[34m2021-01-21 10:46:04,410 - bert_train - INFO - Validation set result details: 0.9071100917431193 \u001b[0m\n",
      "\u001b[34m2021-01-21 10:46:04,410 - bert_train - INFO - Checkpoint model to /opt/ml/checkpoints/checkpoint.pt\u001b[0m\n",
      "\u001b[34m2021-01-21 10:46:05,069 - bert_train - INFO - Run   9257     3     33676     6/8419        0% 0.009600 0.034845       0.9735       0.9071\u001b[0m\n",
      "\u001b[34m###score: train_loss### 0.00959975635956717\u001b[0m\n",
      "\u001b[34m###score: val_loss### 0.03484531511219406\u001b[0m\n",
      "\u001b[34m###score: train_score### 0.9734814176899435\u001b[0m\n",
      "\u001b[34m###score: val_score### 0.9071100917431193\u001b[0m\n",
      "\u001b[34m2021-01-21 11:24:28,120 - bert_train - INFO - Train set result details:\u001b[0m\n",
      "\u001b[34m2021-01-21 11:24:28,161 - bert_train - INFO - Train set result details: 0.9784852039376977\u001b[0m\n",
      "\u001b[34m2021-01-21 11:24:28,162 - bert_train - INFO - Validation set result details:\u001b[0m\n",
      "\u001b[34m2021-01-21 11:24:38,092 - bert_train - INFO - Validation set result details: 0.9231651376146789 \u001b[0m\n",
      "\u001b[34m2021-01-21 11:24:38,092 - bert_train - INFO - Snapshotting because the current score 0.9231651376146789 is greater than 0.9208715596330275 \u001b[0m\n",
      "\u001b[34m2021-01-21 11:24:38,092 - bert_train - INFO - Snapshot model to /opt/ml/model/best_snaphsotmodel.pt\u001b[0m\n",
      "\u001b[34m2021-01-21 11:24:38,750 - bert_train - INFO - Checkpoint model to /opt/ml/checkpoints/checkpoint.pt\u001b[0m\n",
      "\u001b[34m2021-01-21 11:24:39,404 - bert_train - INFO - Run  11571     4     42095     6/8419        0% 0.007560 0.037295       0.9785       0.9232\u001b[0m\n",
      "\u001b[34m###score: train_loss### 0.007560164679325293\u001b[0m\n",
      "\u001b[34m###score: val_loss### 0.03729506027500291\u001b[0m\n",
      "\u001b[34m###score: train_score### 0.9784852039376977\u001b[0m\n",
      "\u001b[34m###score: val_score### 0.9231651376146789\u001b[0m\n",
      "\u001b[34m2021-01-21 12:03:03,955 - bert_train - INFO - Train set result details:\u001b[0m\n",
      "\u001b[34m2021-01-21 12:03:03,997 - bert_train - INFO - Train set result details: 0.9830435492731889\u001b[0m\n",
      "\u001b[34m2021-01-21 12:03:03,997 - bert_train - INFO - Validation set result details:\u001b[0m\n",
      "\u001b[34m2021-01-21 12:03:13,970 - bert_train - INFO - Validation set result details: 0.9151376146788991 \u001b[0m\n",
      "\u001b[34m2021-01-21 12:03:13,970 - bert_train - INFO - Checkpoint model to /opt/ml/checkpoints/checkpoint.pt\u001b[0m\n",
      "\u001b[34m2021-01-21 12:03:14,615 - bert_train - INFO - Run  13886     5     50514     6/8419        0% 0.005999 0.035600       0.9830       0.9151\u001b[0m\n",
      "\u001b[34m###score: train_loss### 0.005999425647223375\u001b[0m\n",
      "\u001b[34m###score: val_loss### 0.03559954065419392\u001b[0m\n",
      "\u001b[34m###score: train_score### 0.9830435492731889\u001b[0m\n",
      "\u001b[34m###score: val_score### 0.9151376146788991\u001b[0m\n",
      "\u001b[34m2021-01-21 12:41:35,450 - bert_train - INFO - Train set result details:\u001b[0m\n",
      "\u001b[34m2021-01-21 12:41:35,492 - bert_train - INFO - Train set result details: 0.9860725474765772\u001b[0m\n",
      "\u001b[34m2021-01-21 12:41:35,492 - bert_train - INFO - Validation set result details:\u001b[0m\n",
      "\u001b[34m2021-01-21 12:41:45,406 - bert_train - INFO - Validation set result details: 0.9220183486238532 \u001b[0m\n",
      "\u001b[34m2021-01-21 12:41:45,406 - bert_train - INFO - Checkpoint model to /opt/ml/checkpoints/checkpoint.pt\u001b[0m\n",
      "\u001b[34m2021-01-21 12:41:46,022 - bert_train - INFO - Run  16198     6     58933     6/8419        0% 0.004739 0.038312       0.9861       0.9220\u001b[0m\n",
      "\u001b[34m###score: train_loss### 0.004739375051080317\u001b[0m\n",
      "\u001b[34m###score: val_loss### 0.0383120624972251\u001b[0m\n",
      "\u001b[34m###score: train_score### 0.9860725474765772\u001b[0m\n",
      "\u001b[34m###score: val_score### 0.9220183486238532\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2021-01-21 13:20:08,860 - bert_train - INFO - Train set result details:\u001b[0m\n",
      "\u001b[34m2021-01-21 13:20:08,902 - bert_train - INFO - Train set result details: 0.987928551277673\u001b[0m\n",
      "\u001b[34m2021-01-21 13:20:08,902 - bert_train - INFO - Validation set result details:\u001b[0m\n",
      "\u001b[34m2021-01-21 13:20:18,808 - bert_train - INFO - Validation set result details: 0.9220183486238532 \u001b[0m\n",
      "\u001b[34m2021-01-21 13:20:18,809 - bert_train - INFO - Checkpoint model to /opt/ml/checkpoints/checkpoint.pt\u001b[0m\n",
      "\u001b[34m2021-01-21 13:20:19,444 - bert_train - INFO - Run  18511     7     67352     6/8419        0% 0.004116 0.038283       0.9879       0.9220\u001b[0m\n",
      "\u001b[34m###score: train_loss### 0.004115670372123552\u001b[0m\n",
      "\u001b[34m###score: val_loss### 0.038282905756538615\u001b[0m\n",
      "\u001b[34m###score: train_score### 0.987928551277673\u001b[0m\n",
      "\u001b[34m###score: val_score### 0.9220183486238532\u001b[0m\n",
      "\u001b[34m2021-01-21 13:58:45,962 - bert_train - INFO - Train set result details:\u001b[0m\n",
      "\u001b[34m2021-01-21 13:58:46,003 - bert_train - INFO - Train set result details: 0.9895618346226374\u001b[0m\n",
      "\u001b[34m2021-01-21 13:58:46,003 - bert_train - INFO - Validation set result details:\u001b[0m\n",
      "\u001b[34m2021-01-21 13:58:55,950 - bert_train - INFO - Validation set result details: 0.911697247706422 \u001b[0m\n",
      "\u001b[34m2021-01-21 13:58:55,950 - bert_train - INFO - Checkpoint model to /opt/ml/checkpoints/checkpoint.pt\u001b[0m\n",
      "\u001b[34m2021-01-21 13:58:56,567 - bert_train - INFO - Run  20828     8     75771     6/8419        0% 0.003344 0.045738       0.9896       0.9117\u001b[0m\n",
      "\u001b[34m###score: train_loss### 0.0033437928366645718\u001b[0m\n",
      "\u001b[34m###score: val_loss### 0.045738447290939205\u001b[0m\n",
      "\u001b[34m###score: train_score### 0.9895618346226374\u001b[0m\n",
      "\u001b[34m###score: val_score### 0.911697247706422\u001b[0m\n",
      "\u001b[34m2021-01-21 13:58:56,568 - bert_train - INFO - Early stopping.. with no improvement in 4\u001b[0m\n",
      "\u001b[34m2021-01-21 13:58:57,264 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2021-01-21 13:59:01 Uploading - Uploading generated training model\n",
      "2021-01-21 13:59:58 Completed - Training job completed\n",
      "Training seconds: 21025\n",
      "Billable seconds: 6307\n",
      "Managed Spot Training savings: 70.0%\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "estimator = PyTorch(\n",
    "                    entry_point='main.py',\n",
    "                    source_dir = 'src',\n",
    "                    role=role,\n",
    "                    framework_version =\"1.4.0\",\n",
    "                    py_version='py3',\n",
    "                    instance_count=1,\n",
    "                    instance_type=instance_type,\n",
    "                    hyperparameters = hp,\n",
    "                    output_path=s3_output_path,\n",
    "                    metric_definitions=metric_definitions,\n",
    "                    volume_size=30,\n",
    "                    code_location=s3_code_path,\n",
    "                    debugger_hook_config=False,\n",
    "                    base_job_name =base_name,  \n",
    "                    use_spot_instances = use_spot,\n",
    "                    max_run =  train_max_run_secs,\n",
    "                    max_wait = max_wait_time_secs,   \n",
    "                    checkpoint_s3_uri=s3_checkpoint,\n",
    "                    checkpoint_local_path=sm_localcheckpoint_dir)\n",
    "\n",
    "estimator.fit(inputs, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy BERT model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference container\n",
    "Ideally the server containing should already have all the required dependencies installed to reduce start up time and ensure that the runtime enviornment is consistent. This can be implemented using a custom docker image.\n",
    "\n",
    "But for this demo, to simplify, we will let the Pytorch container script model install the dependencies during start up. As a result, you will see some of the initial ping requests fail, until all dependencies are installed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-22 01:03:54,598 - sagemaker - INFO - Creating model with name: pytorch-inference-2021-01-21-14-03-54-597\n",
      "2021-01-22 01:03:58,130 - sagemaker - INFO - Creating endpoint with name pytorch-inference-2021-01-21-14-03-56-185\n",
      "----------------!"
     ]
    }
   ],
   "source": [
    "from sagemaker.pytorch import PyTorchModel\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "model_uri = estimator.model_data\n",
    "\n",
    "model = PyTorchModel(model_data=model_uri,\n",
    "                     role=role,\n",
    "                     py_version = \"py3\",\n",
    "                     framework_version='1.4.0',\n",
    "                     entry_point='serve.py',\n",
    "                     source_dir='src')\n",
    "\n",
    "predictor = model.deploy(initial_instance_count=1, instance_type='ml.p3.2xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoke API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "class TextSerDes:\n",
    "    \n",
    "     def serialize(self, x):\n",
    "        data_bytes=\"\\n\".join(x).encode(\"utf-8\")\n",
    "        return data_bytes\n",
    "    \n",
    "     def deserialize(self, x, content_type):\n",
    "        payload =   x.read().decode(\"utf-8\")\n",
    "        return json.loads(payload) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_predict(predictor, data, chunk_size=50):\n",
    "    predictor.serializer = TextSerDes()\n",
    "    predictor.deserializer = TextSerDes()\n",
    "    \n",
    "    result = []\n",
    "    for i in range(0, len(data), chunk_size):\n",
    "        \n",
    "        re = predictor.predict(data[i:i+chunk_size],  initial_args={ \"Accept\":\"text/json\", \"ContentType\" : \"text/csv\" })\n",
    "        result.extend(re)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from s3_util import S3Util\n",
    "import csv, io\n",
    "\n",
    "def load_test_csv(s3_uri):\n",
    "    os.path.join(processed_out_dir, \"test.csv\")\n",
    "    data = S3Util().download_object(s3_uri).decode(\"utf-8\")\n",
    "    \n",
    "    csv_reader = csv.reader(io.StringIO(data), delimiter='\\t',\n",
    "                            quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    \n",
    "    actuals =[]\n",
    "    inputs = []\n",
    "    ids = []\n",
    "    \n",
    "    next(csv_reader)\n",
    "    for r in  csv_reader:\n",
    "        id = r[0]\n",
    "        text = r[1]\n",
    "        \n",
    "        inputs.append(text)\n",
    "        ids.append(id)\n",
    "    return inputs,  ids\n",
    "        \n",
    "\n",
    "def write_predictions_csv( predictions, output_file, additional):\n",
    "    \n",
    "    with open(output_file, \"w\") as f:\n",
    "        csv_writer = csv.writer(f, delimiter='\\t',\n",
    "                            quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "        \n",
    "        csv_writer.writerow([\"prediction\", \"data\"])\n",
    "        for p,a in zip(predictions, additional):\n",
    "            csv_writer.writerow([p, a])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 198 ms, sys: 24.7 ms, total: 223 ms\n",
      "Wall time: 39.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "test_data, ids = load_test_csv(s3_uri_test)\n",
    "response  = chunk_predict(predictor, test_data )\n",
    "predictions_label = [ list(l.keys())[0] for l in response ]\n",
    "\n",
    "write_predictions_csv( predictions_label, \"sst2-output.csv\",test_data )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(test_data) == len(predictions_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-22 09:18:58,456 - sagemaker - INFO - Deleting endpoint configuration with name: pytorch-inference-2021-01-21-14-03-56-185\n",
      "2021-01-22 09:18:59,952 - sagemaker - INFO - Deleting endpoint with name: pytorch-inference-2021-01-21-14-03-56-185\n"
     ]
    }
   ],
   "source": [
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
